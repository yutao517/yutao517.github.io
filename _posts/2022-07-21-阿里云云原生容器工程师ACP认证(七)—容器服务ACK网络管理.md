---
layout: article
title: 阿里云云原生容器工程师ACP认证(七)—容器服务ACK网络管理
tags: 阿里云云原生容器工程师 Kubernetes Docker
category: blog
date: 2022-07-21 15:53:00 +08:00
mermaid: true
---
## 容器集群网络设计目标
**集群网络是Kubernetes的核心部分，它需要解决下面四个核心网络问题**
- 容器与容器之间的通信
- Pod与Pod之间的通信
- Pod与Service之间的通信
- 外部世界与Service之间的通信

## Kubernetes网络接入的三个基本原则
- Pod无论运行在任何节点都可以互相直接通信，而不需要借助NAT地址转换实现。
- Node与Pod可以互相通信，在不限制的前提下，Pod可以访问任意网络。
- Pod拥有独立的网络栈，Pod看到自己的地址和外部看见的地址应该是一样的，并且同个Pod内所有的容器共享同个网络栈。


## 容器到容器之间的通信

- 在容器中,容器之间的网络通信实现是通过docker0网桥，凡是连接到docker0的容器，就可以通过它来进行通信。

- 要想容器能够连接到docker0网桥，我们也需要类似网线的虚拟设备Veth Pair来把容器连接到网桥上。


当容器在一台宿主机上，访问该宿主机上的容器的IP地址时,这个请求的数据包:

- 先根据路由规则到达docker0网桥
- 然后,被转发到对应的Veth Pair设备
- 最后,出现在容器里



## 通过Service实现内外部统一访问
**在Kubernetes集群中为什么需要服务发现(Service)?**

- Kubernetes集群应用是通过Pod部署的，我们知道Pod的生命周期过程中,，随时可能销毁或者重新部署变化
- 应用创建Pod组需要统一访问入口以及负载均衡。

![在这里插入图片描述](https://img-blog.csdnimg.cn/fca438fe114d437d80fdbd1637de7f01.png)


- 应用服务需要暴露到外部去访问，需要提供给用户通过外部网络进行访问
- Pod之间的网络通信也可以通过Kubernetes Service实现访问
- Kubernetes Service可以实现访问负载到一组 Pod上面.

## 跨主机网络通信
在Docker的默认配置下,不同宿主机上的容器通过IP地址进行互相访问是根本做不到的。Kubernetes为了更好的控制网络的接入,推出了CNI即容器网络的API接口。
- 它是Kubernetes中标准的一个调用网络实现的接口
- kubelet通过这个API来调用不同的网络插件以实现不同的网络配置,实现了这个接口的就是CNI插件,它实现 了一系列的CNI API接口。

目前比较常用的Terway、Flannel、Calico、Weave等等。

- CNI，是Kubernetes与底层网络插件之间的一个 抽象层，为k8s屏蔽了底层网络实现的复杂度， 同时解耦了k8s的具体网络插件实现。

- CNI,目前是Kubernetes官方社区推荐的容器网络方案。

## Kubernetes集群网络插件方案

- **Flannel**：Flannel是最早CoreOS团队开源的网络插件，用于让集群中不同节点创建的容器都具有集群内全局唯一的网络 (集群外无法感知)，也是当前Kubernetes开源方案中比较成熟的方案,支持HostGW和VXLAN模式。

 - **Calico**: Calico是一个纯3层的数据中心网络方案，支持IPIP和BGP模式，后者可以无缝集成像OpenStack这种IaaS云架构，能够提供可控的VM、容器、裸机之间的IP通信。但是，需要网络设备对BGP的支持(阿里云vpc子网内应该是不支持BGP 的)。同时，可以支持基于iptables的网络策略控制。
 
- **Contiv**: Contiv是思科开源的用于跨虚拟机、裸机、公有云或私有云的异构容器部署的开源容器网络架构，可支持2层、3 层网络(通常也需要BGP的支持)

- **Terway**: Terway是阿里云开源的基于VPC网络的CNI插件，支持VPC和虚拟网卡ENI模式，后者可实现容器网络使用vpc子网网络。

## 阿里云ACK容器网络技术实现
**CNI**是Container Network Interface的缩写：它是一个通用的容器网络插件的Kubernetes网络接口；开源社区里已经有了很多实现容器网络的方案，不同的网络实现方案在k8s内部都是以插件调用的形式工作，所以这里需要一个统一的标准接口。
- Kubernetes使用CNI接口调用网络插件
- 网络插件(CNI)能分配且唯一的IP地址
- 网络插件(CNI)可以配置Pod的网络和打通Pods之间的访问

## 阿里云容器服务ACK容器网络模式
容器服务将Kubernetes网络、阿里云VPC、阿里云SLB进行深度集成，提供了稳定高性能的容器网络。在容器服务中，支持以下类型的互联互通：
- 同一个容器集群中，Pod之间相互访问
- 同一个容器集群中，Pod访问Service
- 同一个容器集群中，ECS访问Service
- Pod直接访问同一个VPS下的ECS
- 同一个VPC下的ECS直接访问Pod


## 阿里云容器服务ACK容器网络规划与实现

- 在创建ACK Kubernetes集群时，您需要指定专有网络VPC、虚拟交换机、 Pod网络CIDR(地址段)和Service CIDR(地址段)。
- 建议您提前规划ECS地址、Kubernetes Pod地址和Service地址。

![在这里插入图片描述](https://img-blog.csdnimg.cn/f731a0cd7a804501871cb44ca4af7143.png)


## 阿里云容器服务ACK容器网络规划注意事项
**VPC网段**

- 在创建VPC选择的地址段,只能从10.0.0.0/8，172.16.0.0/12，192.168.0.0/16三者当中选择一个

**交换机网段**

- 在VPC里创建交换机时指定的网段,必须是当前VPC网段的子集(可以跟VPC网段地址一样,但不能超过)。

- 交换机下面的ECS所分配到的地址，就是从这个交换机地址段内获取的。

- 一个VPC下，可以创建多个交换机，但交换机网段不能重叠。

**Pod地址段**

- Pod是Kubernetes内的概念,每个Pod具有一个IP地址。

- 在阿里云容器服务上创建Kubernetes集群时，可以指定Pod的地址段，能和VPC网段重叠。

**Service地址段**

- Service也是Kubernetes内的概念，每个Service有自己的地址。

- Service地址段也不能和VPC地址段重合，而且Service地址段也不能和Pod地址段重合。

- Service地址只在Kubernetes集群内使用，不会出集群。

## 容器网络规划-VPC专有网络

**专有网络VPC帮助您基于阿里云构建出一个隔离的网络环境**

- 可以完全掌控自己的虚拟网络，包括选择自有IP地址范围、划分网段、配置路由表和网关等。
- 容器服务通过配置VPC路由表的方式将容器对容器的访问转发到容器IP网段所对应的ECS机器上。
- 容器服务是依赖VPC的路由表做容器IP到ECS的流量转发。

- 在VPC的路由表配置中,我们可以看到容器服务配置的网段到ECS的配置，这个是容器服务自动完成的，如果配置不小心被删除掉了，可以对照节点上的 docker info找到本节点上对应的网段，手动恢复到 VPC的路由表中。


## 容器网络规划-VPC专有网络规划

**阿里云容器网络与阿里云VPC专有网络深度集成**

- 在创建阿里云容器集群之前,首先必须创建专有网络VPC和制定虚拟换机
- 需要结合具体的业务来规划VPC和交换机的数量及网段等

![在这里插入图片描述](https://img-blog.csdnimg.cn/d910f30deec14ff6b04eb91251218a55.png)

## 容器网络规划-容器网络插件

**在容器服务ACK集群中，可以通过两种网络插件实现上述容器网络的能力:**

- Flannel网络插件
- Terway网络插件


## 容器网络规划-Flannel网络插件
Flannel 网络方案是目前使用最为普遍的Kubernetes网络方案，使用的是简单稳定的社区Flannel CNI插件。
- 配合阿里云的VPC的高速网络,能给集群高性能和稳定的容器网络体验。
- Flannel功能偏简单，支持的特性少。例如，不支持基于Kubernetes标准的网络策略（Network Policy）

- 一个典型的容器网方案。它首先要解决的是 container 的包如何到达 Host，这里采用的是加一个 Bridge 的方式。并且通过独立Backend将数据包传送出去离开Host,，实现不同POD之间通信。

- Backend 可以通过三种后端机制进行转发：VXLAN、UDP、host-gw

![在这里插入图片描述](https://img-blog.csdnimg.cn/126a7d33a3c449c8b540241bbe016504.png)

## 容器网络规划-Flannel网络模式
 Flannel网络模式中Pod的网段独立于VPC的网段。Pod网段会按照掩码均匀划分给每个集群中的节点，每个节点上的Pod会从节点上划分的 网段中分配IP地址。
- 基于阿里云VPC的Flannel网络无封包，相对默认的Flannel VXLAN性能提升20%.
- Pod网段是独立于VPC的虚拟网段，每个节点需要对应一个VPC的路由表条目。

## 容器网络规划-Terway网络插件

Terway是阿里云开源的基于专有网络VPC的容器网络接口CNI (Container Network Interface) 插件。

- 支持基于Kubernetes标准的网络策略来定义容器间的访问策略
- 将原生的弹性网卡分配给Pod实现Pod网络
- 支持基于Kubernetes标准的网络策略(Network Policy)来定义容器间的访问策略，并兼容Calico的网络策略。
- Terway所提供的VPC互通的网络方案,
方便对接已有的基础设施，同时,没有overlay网络封包解包的性能损耗，简单易用，出现网络问题方便诊断。

## 容器网络规划-Terway网络模式
Terway网络模式采用的是云原生的网络方案，直接基于阿里云的虚拟化网络中的弹性网卡资源来构建的容器网络。Pod会通过弹性网卡资源直接分配VPC中的IP地址，而不需要额外指定虚拟Pod网段。
- 在Terway网络模式中，每个Pod都拥有自己网络栈和IP地址。
- 同一台ECS内的Pod之间通信，直接通过机器内部转发，跨ECS的Pod通信、报文通过VPC的弹性网卡直接转发。
- 由于不需要使用VXLAN等的隧道技术封装报文，因此Terway模式网络具有较高的通信性能。

## Terway与Flannel对比
在创建集群时，ACK提供Terway和Flannel两种网络插件：

- Terway：是阿里云容器服务ACK自研的网络插件。Terway将阿里云的弹性网卡分配给容器，支持基于Kubernetes标准的网络策略来定义容器间的访问策略，支持对单个容器做带宽的限流。Terway拥有更为灵活的IPAM（容器地址分配）策略，避免地址浪费。如果您不需要使用网络策略（Network Policy），可以选择Flannel，其他情况建议选择Terway。
- Flannel：使用的是简单稳定的社区Flannel CNI插件。配合阿里云的VPC的高速网络，能给集群高性能和稳定的容器网络体验。Flannel功能偏简单，支持的特性少。例如，不支持基于Kubernetes标准的网络策略（Network Policy）。更多信息，请参见Flannel。

![在这里插入图片描述](https://img-blog.csdnimg.cn/5e4987e3f1824cdbac6546bb08664478.png)

## 容器网络规划-Service网络

- 通过服务(Service)抽象,能够解耦前端和后端的关联，从而实现松耦合的微服务设计,以及自动负载均衡实现快速的业务弹性。
- ACK容器服务采用Service方式为一组容器提供固定的访问入口，并对这一组容器做负载均衡。
![在这里插入图片描述](https://img-blog.csdnimg.cn/522e2bacdcaf47649dc81ba29e75ee2a.png)

## 容器网络规划-Ingress
- 在Kubernetes集群中，Ingress作为集群内服务对外暴露的访问接入点,其几乎承载着集群内服务访问的所有流量。
- Ingress是Kubernetes中的一个资源对象，用来管理集群外部访问集群内部服务的方式。
- 通过Ingress资源来配置不同的转发规则,从而达到根据不同的规则设置访问集群内不同的Service 后端Pod。

阿里云容器服务提供高可靠的Ingress Controller组件，集成了阿里云SLB服务,为您的Kubernetes集群提供灵活可靠的路由服务(Ingress)。

![在这里插入图片描述](https://img-blog.csdnimg.cn/3706863aaeaa4b4ebc2af7126297d08c.png)
常见的前后端分离的架构方式中，前后端的访问地址分别使用不同的访问路径。对应这种场景，可以采用Ingress，根据7层的访问路径负载到不同的应用实例上。

![在这里插入图片描述](https://img-blog.csdnimg.cn/71f4d8d70c9b4309b5c62058a62f1cb6.png)

## 容器网络规划-容器网络搭建完成

ACK容器网络搭建包括：容器网络规划、VPC专有网络配置阶段、容器网络配置阶段、 Service网络配置阶段。根据集群业务需求，实现容器网络的配置完成。

- 容器与容器之间的通信
- Pod与Pod之间的通信
-  Pod与Service之间的通信
- 外部世界与Service之间的通信

## 容器网络最佳实践: 选用Flannel网络插件实现容器网络
- 本实践在首先创建一个VPC的基础环境(包括一套VPC专有网网络、以及三个不同区域的交换机)
- 基于VPC基础环境，创建Flannel网络模式类型的ACK容器集群，并在集群上构建一个nginx应用，并实现服务访问。

**1.创建专有网络**

- 步骤1：登录阿里云管理控制台。
- 步骤2：通过产品与服务导航，定位到专有网络VPC,，单击进入专有网络控制台。
- 步骤3：在专有网络页面，将地域设置为华东2(上海)，并单击创建专有网络。
- 步骤4：在创建专有网络侧边页面，完成以下配置，并单击确定。

  例：
![在这里插入图片描述](https://img-blog.csdnimg.cn/f1d3cc50208c4f3485f229768e465d68.png)
- 步骤5：等待专有网络和交换机创建成功，单击完成。

**2. 创建跨可用区普通ACK集群**
- 步骤1：登录阿里云管理控制台。
- 步骤2：通过产品与服务导航，定位到容器服务ACK，单击进入容器服务控制台。
- 步骤3：前往集群页面，单击创建集群。
- 步骤4：在选择集群模板页面，单击标准托管集群下的创建。
- 步骤5：在创建Kubernetes集群页面，完成以下配置，并单击创建集群。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/00eb7cb634f24c4094a61b93d7050421.png)
- 步骤6：在创建Kubernetes集群页面，完成Worker实例配置，并单击创建集群。
- 步骤7：等待集群创建完成。该过程约需10分钟。

**3. 创建应用**

- 步骤1：回到容器服务控制台。
- 步骤2：前往应用>无状态页面，将集群设置为cluster-standard，并单击使用镜像创建。
- 步骤3:填写应用基本信息，并单击下一步。应用基本信息的描述见下表。

  ![](https://img-blog.csdnimg.cn/97aa451a4f78428789268f6ed4c8f329.png)
- 步骤4：完成容器配置。
在镜像名称后，单击选择镜像。
在镜像选择对话框中，选择nginx镜像，并单击确定。
在镜像版本后，单击选择镜像版本。
选择latest版本，并单击确定。
下一步进行容器高级配置。
- 步骤5：完成高级配置。
在服务 (Service)后，单击创建。
在创建服务对话框中，完成以下配置，并单击创建。

  ![在这里插入图片描述](https://img-blog.csdnimg.cn/b86d05ae477446d591ea9d8e238f22fe.png)
在容器组水平伸缩下，根据您的需要选择是否开启伸缩。如果是创建正式使用的应用，需要开启伸缩。本实践简化应用配置，选择不开启伸缩。
- 步骤6：等待创建应用任务完成。

## 容器网络最佳实践:选用Terway插件实现容器网络
- 本实践在首先创建一个VPC的基础环境基于VPC基础环境(创建一个专有网络和两个虚拟交换机)
- 在创建ACK容器集群时，选择Terway网络模式，并配置Terway网络参数，实现基于Terway插件实现容器集群创建。

**Terway网络模式规划注意事项：**

- 如果要使用Terway插件，建议您选择较高规格和较新类型的ECS机型，即5代以后、8核以上的机型。
- 单节点所支持的最大Pod数取决于该节点的弹性网卡（ENI）数。
  - 共享ENI支持的最大Pod数=（ECS支持的ENI数-1）×单个ENI支持的私有IP数
  - 独占ENI支持的最大Pod数=ECS支持的ENI数－1

**步骤一:规划和准备集群网络**
- 在创建ACK Kubernetes集群时，您需要指定专有网络VPC、虚拟交换机、Pod虚拟交换机和Service CIDR (地址段)
- 如果使用Terway网络插件，您需要先创建在一个专有网络VPC，然后在VPC下创建两个虚拟交换机。
- 这里面虚拟交换机和Pod虚拟交换机需要在一个可用区下。

  ![](https://img-blog.csdnimg.cn/362be55197b64547815a4743c20d1be8.png)

**步骤二:专有网络搭建**
- 1.登录专有网络管理控制台。
- 2.在顶部菜单栏处，选择专有网络的地域，然后单击创建专有网络。
- 3.创建专有网络和创建交换机：
![](https://img-blog.csdnimg.cn/c1bfa1037c2f4eca93b83e95142339de.png)
（专有网络的地域和要部署的云资源的地域必须相同）


**步骤三:创建ACK集群**
- 1.登录阿里云管理控制台。
- 2.通过产品与服务导航，定位到容器服务ACK，单击进入容器服务控制台。
- 3.前往集群页面，单击创建集群。
- 4.在选择集群模板页面，单击标准托管集群下的创建。

**步骤四:配置Terway网络**

为Terway网络插件配置集群网络的关键参数配置说明如下:
- 专有网络：选择在步骤1已创建的专有网络名称。
- 虚拟交换机：选择步骤1创建的虚拟交换机名称。
- 网络插件：选择Terway。

设置网络插件为Terway时，需要配置Terway模式：
- Terway网络插件，支持独占弹性网卡和Ipvlan(弹性网卡共享模式)两种模式。
- NetworkPolicy支持，只在弹性网卡共享模式下支持选中,默认不选中。
- Pod虚拟交换机：选择选择步骤1中创建的虚拟交换机名称。
- Service CIDR：保留默认值。
![在这里插入图片描述](https://img-blog.csdnimg.cn/b69ea55b27cc437aa92621b1c1fdad99.png)


独占弹性网卡注意说明 当前只有白名单用户可使用上述 Pod独占弹性网卡以获得最佳性能功能。提交工单申请使用。

**步骤五:配置节点实例及创建集群。**
- 选择节点实例规格时，需要考虑其支持挂载的ENI数量和相应的Pod 数量。
- 在规格列表中，相应的数据已经列出，绿色数字为该型号在不同 Terway模式下可支持的Pod数量。
- 其他配置基本与Flannel模式集群创建类似,完成配置后,开始创建集群。

**Terway 集群配置与 Flannel 集群相比主要有两点不同:**

- Pod 需要通过弹性网卡(ENI)接入虚拟交换机，来组建 Pod 网络。
- 不同 ECS 规格所能支持挂载的弹性网卡数量不同。 .
